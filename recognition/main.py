#!/usr/bin/env python3
"""
Recognition Service - –†–µ—Ñ–∞–∫—Ç–æ—Ä–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è
–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: Quality Check ‚Üí Preprocessing ‚Üí InsightFace ‚Üí Tracking ‚Üí Presence Logic
"""

import time
import os
import io
import pickle
import hashlib
import threading
import logging
from typing import Optional, Tuple, Dict, List
from dataclasses import dataclass
from flask import Flask, Response
from flask_cors import CORS

import cv2
import numpy as np
import requests
from insightface.app import FaceAnalysis


# ============================================================================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
# ============================================================================

@dataclass
class Config:
    """–¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã"""
    
    # Backend
    backend_url: str = os.getenv('BACKEND_URL', 'http://localhost:3000')
    
    # Camera
    camera_source: str = os.getenv('CAMERA_SOURCE', '0')
    frame_skip: int = int(os.getenv('FRAME_SKIP', '3'))
    
    # Quality thresholds
    min_face_height_pixels: int = int(os.getenv('MIN_FACE_HEIGHT', '20'))
    min_blur_variance: float = float(os.getenv('MIN_BLUR_VAR', '50.0'))
    
    # Preprocessing
    enable_preprocessing: bool = os.getenv('ENABLE_PREPROCESSING', 'true').lower() == 'true'
    clahe_clip_limit: float = float(os.getenv('CLAHE_CLIP', '2.0'))
    denoise_strength: int = int(os.getenv('DENOISE_STRENGTH', '5'))
    
    # InsightFace
    insightface_threshold: float = float(os.getenv('INSIGHTFACE_THRESHOLD', '0.2'))
    insightface_det_size: Tuple[int, int] = (640, 640)
    
    # Tracking
    min_good_embeddings_per_track: int = int(os.getenv('MIN_EMBEDDINGS', '2'))
    track_max_age_seconds: float = float(os.getenv('TRACK_MAX_AGE', '2.0'))
    iou_threshold: float = 0.3
    
    # Presence logic
    in_threshold_seconds: float = float(os.getenv('IN_THRESHOLD', '1.0'))
    out_threshold_seconds: float = float(os.getenv('OUT_THRESHOLD', '10.0'))
    
    # System
    reload_employees_interval: int = int(os.getenv('RELOAD_INTERVAL', '300'))
    video_stream_port: int = int(os.getenv('VIDEO_PORT', '5001'))
    cache_file: str = 'face_encodings_cache.pkl'
    
    # Logging
    debug_mode: bool = os.getenv('DEBUG', 'true').lower() == 'true'


config = Config()

# ============================================================================
# –õ–û–ì–ò–†–û–í–ê–ù–ò–ï
# ============================================================================

logging.basicConfig(
    level=logging.DEBUG if config.debug_mode else logging.INFO,
    format='[%(levelname)s] %(message)s'
)
logger = logging.getLogger('recognition')


# ============================================================================
# –ì–õ–û–ë–ê–õ–¨–ù–´–ï –ü–ï–†–ï–ú–ï–ù–ù–´–ï
# ============================================================================

current_frame = None
frame_lock = threading.Lock()

# Flask –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
app = Flask(__name__)
CORS(app)

# InsightFace –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
logger.info('Initializing InsightFace AI...')
face_app = FaceAnalysis(providers=['CPUExecutionProvider'])
face_app.prepare(ctx_id=0, det_size=config.insightface_det_size)
logger.info(f'‚úÖ InsightFace initialized (det_size={config.insightface_det_size})')


# ============================================================================
# 1. –û–¶–ï–ù–ö–ê –ö–ê–ß–ï–°–¢–í–ê –õ–ò–¶–ê
# ============================================================================

def compute_blur_score(gray_face: np.ndarray) -> float:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫—É —Ä–∞–∑–º—ã—Ç–∏—è —á–µ—Ä–µ–∑ –≤–∞—Ä–∏–∞—Ü–∏—é –õ–∞–ø–ª–∞—Å–∏–∞–Ω–∞.
    –ß–µ–º –≤—ã—à–µ –∑–Ω–∞—á–µ–Ω–∏–µ - —Ç–µ–º —Ä–µ–∑—á–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.
    """
    return cv2.Laplacian(gray_face, cv2.CV_64F).var()


def is_face_acceptable(face_img_bgr: np.ndarray, bbox: np.ndarray) -> Tuple[bool, Dict]:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –ª–∏—Ü–∞ –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è.
    
    Returns:
        (acceptable, metrics) –≥–¥–µ metrics —Å–æ–¥–µ—Ä–∂–∏—Ç height, blur_score –∏ —Ç.–¥.
    """
    try:
        # –ú–µ—Ç—Ä–∏–∫–∞ 1: –†–∞–∑–º–µ—Ä –ª–∏—Ü–∞
        x1, y1, x2, y2 = bbox.astype(int)
        face_height = y2 - y1
        face_width = x2 - x1
        
        # –ú–µ—Ç—Ä–∏–∫–∞ 2: –†–∞–∑–º—ã—Ç–∏–µ
        gray_face = cv2.cvtColor(face_img_bgr, cv2.COLOR_BGR2GRAY)
        blur_score = compute_blur_score(gray_face)
        
        # –ú–µ—Ç—Ä–∏–∫–∞ 3: –Ø—Ä–∫–æ—Å—Ç—å (–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ)
        mean_brightness = np.mean(gray_face)
        
        metrics = {
            'height': face_height,
            'width': face_width,
            'blur_score': blur_score,
            'brightness': mean_brightness
        }
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Ä–æ–≥–æ–≤
        if face_height < config.min_face_height_pixels:
            logger.debug(f'Face rejected: too small ({face_height}px < {config.min_face_height_pixels}px)')
            return False, metrics
        
        if blur_score < config.min_blur_variance:
            logger.debug(f'Face rejected: too blurry (score={blur_score:.1f} < {config.min_blur_variance})')
            return False, metrics
        
        logger.debug(f'Face accepted: h={face_height}px, blur={blur_score:.1f}, bright={mean_brightness:.1f}')
        return True, metrics
        
    except Exception as e:
        logger.error(f'Quality check failed: {e}')
        return False, {}


# ============================================================================
# 2. –ü–†–ï–ü–†–û–¶–ï–°–°–ò–ù–ì –õ–ò–¶–ê
# ============================================================================

def preprocess_face_for_insightface(face_bgr: np.ndarray) -> np.ndarray:
    """
    –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –ª–∏—Ü–æ –¥–ª—è InsightFace:
    - –î–µ–Ω–æ–π–∑–∏–Ω–≥ (–º—è–≥–∫–∏–π)
    - CLAHE (—Ç–æ–ª—å–∫–æ –ø–æ —è—Ä–∫–æ—Å—Ç–∏)
    - Unsharp mask (–º—è–≥–∫–∏–π)
    """
    if not config.enable_preprocessing:
        return face_bgr
    
    try:
        # –®–∞–≥ 1: –ú—è–≥–∫–∏–π –¥–µ–Ω–æ–π–∑–∏–Ω–≥ (—Å–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª–∏)
        denoised = cv2.fastNlMeansDenoisingColored(
            face_bgr, None,
            h=config.denoise_strength,
            hColor=config.denoise_strength,
            templateWindowSize=7,
            searchWindowSize=21
        )
        
        # –®–∞–≥ 2: CLAHE —Ç–æ–ª—å–∫–æ –ø–æ —è—Ä–∫–æ—Å—Ç–∏ (–Ω–µ —Ç—Ä–æ–≥–∞–µ–º —Ü–≤–µ—Ç)
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º YCrCb –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        ycrcb = cv2.cvtColor(denoised, cv2.COLOR_BGR2YCrCb)
        y, cr, cb = cv2.split(ycrcb)
        
        clahe = cv2.createCLAHE(
            clipLimit=config.clahe_clip_limit,
            tileGridSize=(8, 8)
        )
        y = clahe.apply(y)
        
        enhanced = cv2.cvtColor(cv2.merge([y, cr, cb]), cv2.COLOR_YCrCb2BGR)
        
        # –®–∞–≥ 3: –ú—è–≥–∫–∏–π unsharp mask (—Ä–µ–∑–∫–æ—Å—Ç—å –±–µ–∑ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤)
        gaussian = cv2.GaussianBlur(enhanced, (0, 0), 2.0)
        sharpened = cv2.addWeighted(enhanced, 1.5, gaussian, -0.5, 0)
        
        return sharpened
        
    except Exception as e:
        logger.error(f'Preprocessing failed: {e}')
        return face_bgr


# ============================================================================
# 3. –¢–†–ï–ö–ò–ù–ì –õ–ò–¶
# ============================================================================

class FaceTrack:
    """–¢—Ä–µ–∫ –æ–¥–Ω–æ–≥–æ –ª–∏—Ü–∞ –≤ –≤–∏–¥–µ–æ–ø–æ—Ç–æ–∫–µ"""
    
    def __init__(self, track_id: int):
        self.track_id = track_id
        self.embeddings: List[np.ndarray] = []
        self.quality_scores: List[Dict] = []
        self.last_bbox: Optional[np.ndarray] = None
        self.last_update_time: float = time.time()
        self.recognized_employee_id: Optional[int] = None
        self.recognition_confidence: float = 0.0
    
    def add_embedding(self, embedding: np.ndarray, quality: Dict, bbox: np.ndarray):
        """–î–æ–±–∞–≤–ª—è–µ—Ç —Ö–æ—Ä–æ—à–∏–π —ç–º–±–µ–¥–¥–∏–Ω–≥ –≤ —Ç—Ä–µ–∫"""
        self.embeddings.append(embedding)
        self.quality_scores.append(quality)
        self.last_bbox = bbox
        self.last_update_time = time.time()
        logger.debug(f'Track {self.track_id}: added embedding (total: {len(self.embeddings)})')
    
    def is_ready_for_recognition(self) -> bool:
        """–î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è"""
        return len(self.embeddings) >= config.min_good_embeddings_per_track
    
    def get_average_embedding(self) -> np.ndarray:
        """–£—Å—Ä–µ–¥–Ω—ë–Ω–Ω—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥ (–±–æ–ª–µ–µ –Ω–∞–¥—ë–∂–Ω—ã–π —á–µ–º –æ–¥–∏–Ω –∫–∞–¥—Ä)"""
        if not self.embeddings:
            return None
        return np.mean(self.embeddings, axis=0)
    
    def is_alive(self) -> bool:
        """–ñ–∏–≤ –ª–∏ —Ç—Ä–µ–∫ (–Ω–µ–¥–∞–≤–Ω–æ –æ–±–Ω–æ–≤–ª—è–ª—Å—è)"""
        return (time.time() - self.last_update_time) < config.track_max_age_seconds


def compute_iou(bbox1: np.ndarray, bbox2: np.ndarray) -> float:
    """Intersection over Union –¥–ª—è bounding boxes"""
    x1_min, y1_min, x1_max, y1_max = bbox1
    x2_min, y2_min, x2_max, y2_max = bbox2
    
    # –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ
    inter_x_min = max(x1_min, x2_min)
    inter_y_min = max(y1_min, y2_min)
    inter_x_max = min(x1_max, x2_max)
    inter_y_max = min(y1_max, y2_max)
    
    inter_area = max(0, inter_x_max - inter_x_min) * max(0, inter_y_max - inter_y_min)
    
    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ
    bbox1_area = (x1_max - x1_min) * (y1_max - y1_min)
    bbox2_area = (x2_max - x2_min) * (y2_max - y2_min)
    union_area = bbox1_area + bbox2_area - inter_area
    
    if union_area == 0:
        return 0.0
    
    return inter_area / union_area


class FaceTracker:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä —Ç—Ä–µ–∫–æ–≤ –ª–∏—Ü"""
    
    def __init__(self):
        self.tracks: List[FaceTrack] = []
        self.next_track_id = 1
    
    def update(self, faces, known_embeddings, known_ids) -> List[FaceTrack]:
        """
        –û–±–Ω–æ–≤–ª—è–µ—Ç —Ç—Ä–µ–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ª–∏—Ü.
        
        Returns:
            List –∞–∫—Ç–∏–≤–Ω—ã—Ö —Ç—Ä–µ–∫–æ–≤ —Å —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–º–∏ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞–º–∏
        """
        current_time = time.time()
        
        # –£–¥–∞–ª—è–µ–º –º—ë—Ä—Ç–≤—ã–µ —Ç—Ä–µ–∫–∏
        self.tracks = [t for t in self.tracks if t.is_alive()]
        
        # –°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ–º –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ª–∏—Ü–∞ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ —Ç—Ä–µ–∫–∞–º–∏
        matched_track_ids = set()
        
        for face in faces:
            bbox = face.bbox
            embedding = face.normed_embedding
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –ª–∏—Ü–∞
            x1, y1, x2, y2 = bbox.astype(int)
            face_crop = current_frame[y1:y2, x1:x2] if current_frame is not None else None
            
            if face_crop is None or face_crop.size == 0:
                continue
            
            acceptable, quality = is_face_acceptable(face_crop, bbox)
            
            if not acceptable:
                # –ü–ª–æ—Ö–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                continue
            
            # –ò—â–µ–º –ø–æ–¥—Ö–æ–¥—è—â–∏–π —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ç—Ä–µ–∫
            best_track = None
            best_iou = 0.0
            
            for track in self.tracks:
                if track.track_id in matched_track_ids:
                    continue
                if track.last_bbox is None:
                    continue
                
                iou = compute_iou(bbox, track.last_bbox)
                if iou > config.iou_threshold and iou > best_iou:
                    best_iou = iou
                    best_track = track
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ç—Ä–µ–∫ –∏–ª–∏ —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π
            if best_track:
                # –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –ª–∏—Ü–∞
                preprocessed = preprocess_face_for_insightface(face_crop)
                
                best_track.add_embedding(embedding, quality, bbox)
                matched_track_ids.add(best_track.track_id)
                
                # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å –µ—Å–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
                if best_track.is_ready_for_recognition() and not best_track.recognized_employee_id:
                    avg_embedding = best_track.get_average_embedding()
                    emp_id, confidence = match_embedding_to_employee(
                        avg_embedding, known_embeddings, known_ids
                    )
                    
                    if emp_id:
                        best_track.recognized_employee_id = emp_id
                        best_track.recognition_confidence = confidence
                        logger.info(
                            f'Track {best_track.track_id} ‚Üí Employee {emp_id} '
                            f'(confidence: {confidence:.3f}, embeddings: {len(best_track.embeddings)})'
                        )
            else:
                # –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π —Ç—Ä–µ–∫
                new_track = FaceTrack(self.next_track_id)
                self.next_track_id += 1
                
                # –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥
                preprocessed = preprocess_face_for_insightface(face_crop)
                
                new_track.add_embedding(embedding, quality, bbox)
                self.tracks.append(new_track)
                logger.debug(f'Created new track {new_track.track_id}')
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–µ —Ç—Ä–µ–∫–∏
        return [t for t in self.tracks if t.recognized_employee_id is not None]


def match_embedding_to_employee(
    embedding: np.ndarray,
    known_embeddings: List[np.ndarray],
    known_ids: List[int]
) -> Tuple[Optional[int], float]:
    """
    –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ —Å –±–∞–∑–æ–π —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤.
    
    Returns:
        (employee_id, confidence) –∏–ª–∏ (None, 0.0)
    """
    if len(known_embeddings) == 0:
        return None, 0.0
    
    # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ (–¥–ª—è InsightFace normalized embeddings)
    similarities = [np.dot(embedding, known_emb) for known_emb in known_embeddings]
    best_idx = np.argmax(similarities)
    best_similarity = similarities[best_idx]
    
    if best_similarity > config.insightface_threshold:
        return known_ids[best_idx], best_similarity
    
    logger.debug(f'No match: best similarity {best_similarity:.3f} < threshold {config.insightface_threshold}')
    return None, 0.0


# ============================================================================
# 4. –õ–û–ì–ò–ö–ê –ü–†–ò–°–£–¢–°–¢–í–ò–Ø (IN/OUT)
# ============================================================================

class PresenceManager:
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏—è —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤"""
    
    def __init__(self, employee_ids: List[int]):
        self.state = {
            emp_id: {
                'present': False,
                'last_seen': 0.0,
                'last_state_change': 0.0
            }
            for emp_id in employee_ids
        }
    
    def update(self, recognized_employee_ids: List[int]):
        """
        –û–±–Ω–æ–≤–ª—è–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã—Ö –ª–∏—Ü.
        
        Returns:
            List[Tuple[employee_id, event_type]] - —Å–æ–±—ã—Ç–∏—è –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏
        """
        now = time.time()
        events = []
        
        # –û–±–Ω–æ–≤–ª—è–µ–º last_seen –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã—Ö
        for emp_id in recognized_employee_ids:
            if emp_id in self.state:
                self.state[emp_id]['last_seen'] = now
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–æ–≥–∏–∫—É IN/OUT –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞
        for emp_id, state in self.state.items():
            if emp_id in recognized_employee_ids:
                # –°–æ—Ç—Ä—É–¥–Ω–∏–∫ –≤–∏–¥–µ–Ω –≤ –∫–∞–¥—Ä–µ
                if not state['present']:
                    # –ë—ã–ª –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–º, –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Ä–æ–≥
                    time_since_change = now - state['last_state_change']
                    if time_since_change > config.in_threshold_seconds:
                        # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤—Ä–µ–º–µ–Ω–∏ –ø—Ä–æ—à–ª–æ - —Ñ–∏–∫—Å–∏—Ä—É–µ–º –ø—Ä–∏—Ö–æ–¥
                        state['present'] = True
                        state['last_state_change'] = now
                        events.append((emp_id, 'IN'))
                        logger.info(
                            f'‚úÖ Employee {emp_id} marked as IN '
                            f'(stable presence {time_since_change:.1f}s)'
                        )
            else:
                # –°–æ—Ç—Ä—É–¥–Ω–∏–∫ –Ω–µ –≤–∏–¥–µ–Ω
                if state['present']:
                    time_since_seen = now - state['last_seen']
                    if time_since_seen > config.out_threshold_seconds:
                        # –î–∞–≤–Ω–æ –Ω–µ –≤–∏–¥–µ–ª–∏ - —Ñ–∏–∫—Å–∏—Ä—É–µ–º —É—Ö–æ–¥
                        state['present'] = False
                        state['last_state_change'] = now
                        events.append((emp_id, 'OUT'))
                        logger.info(
                            f'‚úÖ Employee {emp_id} marked as OUT '
                            f'(absent {time_since_seen:.1f}s)'
                        )
        
        return events
    
    def add_employee(self, emp_id: int):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–≤–æ–≥–æ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞ –≤ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ"""
        if emp_id not in self.state:
            self.state[emp_id] = {
                'present': False,
                'last_seen': 0.0,
                'last_state_change': 0.0
            }


# ============================================================================
# –£–¢–ò–õ–ò–¢–´
# ============================================================================

def get_employees_hash(employees: List[Dict]) -> str:
    """–•–µ—à —Å–ø–∏—Å–∫–∞ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π"""
    data = ''.join([f"{e['id']}-{e.get('photoUrl', '')}" for e in employees])
    return hashlib.md5(data.encode()).hexdigest()


def save_cache(encodings: List, ids: List, emp_hash: str):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–µ—à —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    try:
        with open(config.cache_file, 'wb') as f:
            pickle.dump({
                'encodings': encodings,
                'ids': ids,
                'hash': emp_hash,
                'timestamp': time.time()
            }, f)
        logger.info(f'Cache saved for {len(ids)} employees')
    except Exception as e:
        logger.error(f'Failed to save cache: {e}')


def load_cache() -> Tuple[Optional[List], Optional[List], Optional[str]]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–µ—à —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    if not os.path.exists(config.cache_file):
        return None, None, None
    
    try:
        with open(config.cache_file, 'rb') as f:
            cache = pickle.load(f)
            age = time.time() - cache.get('timestamp', 0)
            logger.info(f'Cache found (age: {age:.0f} seconds)')
            return cache['encodings'], cache['ids'], cache['hash']
    except Exception as e:
        logger.error(f'Failed to load cache: {e}')
        return None, None, None


def load_employees() -> Tuple[List[np.ndarray], List[int]]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤ –∏–∑ backend –∏ —Å–æ–∑–¥–∞—ë—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏"""
    logger.info('Loading employees from backend...')
    resp = requests.get(f'{config.backend_url}/api/employees', timeout=10)
    resp.raise_for_status()
    employees = resp.json()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à
    current_hash = get_employees_hash(employees)
    cached_encodings, cached_ids, cached_hash = load_cache()
    
    if cached_encodings and cached_hash == current_hash:
        logger.info(f'‚úÖ Using cached encodings for {len(cached_ids)} employees')
        return cached_encodings, cached_ids
    
    logger.info('Cache miss, processing photos...')
    
    known_embeddings = []
    known_ids = []
    
    for emp in employees:
        photo_url = emp.get('photoUrl')
        if not photo_url:
            continue
        
        full_url = config.backend_url + photo_url
        
        try:
            logger.info(f"Processing photo for {emp['name']}...")
            img_resp = requests.get(full_url, timeout=10)
            img_resp.raise_for_status()
            
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            nparr = np.frombuffer(img_resp.content, np.uint8)
            image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
            
            # –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥
            preprocessed = preprocess_face_for_insightface(image)
            
            # InsightFace –¥–µ—Ç–µ–∫—Ü–∏—è –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥
            faces = face_app.get(preprocessed)
            
            if not faces:
                logger.warning(f"No face found for {emp['name']}")
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
            face = faces[0]
            face_crop = image  # –î–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –±–µ—Ä—ë–º –æ—Ä–∏–≥–∏–Ω–∞–ª
            acceptable, quality = is_face_acceptable(face_crop, face.bbox)
            
            if not acceptable:
                logger.warning(
                    f"Face quality too low for {emp['name']}: "
                    f"height={quality.get('height')}px, blur={quality.get('blur_score', 0):.1f}"
                )
                continue
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
            embedding = face.normed_embedding
            known_embeddings.append(embedding)
            known_ids.append(emp['id'])
            
            logger.info(
                f"‚úÖ {emp['name']} - embedding created "
                f"(quality: h={quality.get('height')}px, blur={quality.get('blur_score', 0):.1f})"
            )
            
        except Exception as e:
            logger.error(f"Failed for {emp.get('name', 'unknown')}: {e}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–µ—à
    save_cache(known_embeddings, known_ids, current_hash)
    
    logger.info(f'Loaded {len(known_ids)} employees with photos')
    return known_embeddings, known_ids


def send_event(employee_id: int, event_type: str):
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Å–æ–±—ã—Ç–∏–µ IN/OUT –≤ backend"""
    try:
        logger.info(f'üì§ Sending event {event_type} for employee {employee_id}')
        resp = requests.post(
            f'{config.backend_url}/api/events',
            json={'employeeId': employee_id, 'type': event_type},
            timeout=5
        )
        if not resp.ok:
            logger.error(f'Failed to send event: {resp.status_code} {resp.text}')
    except Exception as e:
        logger.error(f'Error sending event: {e}')


def connect_camera(max_retries=5) -> cv2.VideoCapture:
    """–ü–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è –∫ –∫–∞–º–µ—Ä–µ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞
    if config.camera_source.isdigit():
        camera_type = 'local'
        camera_index = int(config.camera_source)
    elif config.camera_source.startswith('rtsp://') or config.camera_source.startswith('http://'):
        camera_type = 'stream'
        camera_url = config.camera_source
    else:
        camera_type = 'local'
        camera_index = 0
    
    for attempt in range(max_retries):
        if camera_type == 'local':
            logger.info(f'Connecting to local camera {camera_index} (attempt {attempt + 1}/{max_retries})...')
            video_capture = cv2.VideoCapture(camera_index)
        else:
            logger.info(f'Connecting to remote camera (attempt {attempt + 1}/{max_retries})...')
            logger.info(f'URL: {camera_url}')
            video_capture = cv2.VideoCapture(camera_url, cv2.CAP_FFMPEG)
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –∑–∞–¥–µ—Ä–∂–∫–∏ RTSP
            video_capture.set(cv2.CAP_PROP_BUFFERSIZE, 1)
        
        if video_capture.isOpened():
            ret, frame = video_capture.read()
            if ret:
                logger.info(f'‚úÖ Camera connected ({camera_type})')
                logger.info(f'Frame size: {frame.shape[1]}x{frame.shape[0]}')
                
                # –î–ª—è RTSP - —Å–±—Ä–∞—Å—ã–≤–∞–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–π –±—É—Ñ–µ—Ä
                if camera_type == 'stream':
                    logger.info('Flushing initial buffer...')
                    for _ in range(5):
                        video_capture.grab()
                    logger.info('Low latency mode active')
                
                return video_capture
            else:
                video_capture.release()
        
        if attempt < max_retries - 1:
            wait_time = 2 ** attempt
            logger.warning(f'Retry in {wait_time} seconds...')
            time.sleep(wait_time)
    
    raise Exception(f'Cannot connect to camera after {max_retries} attempts')


# ============================================================================
# –í–ò–î–ï–û –°–¢–†–ò–ú
# ============================================================================

def generate_frames():
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∫–∞–¥—Ä–æ–≤ –¥–ª—è MJPEG —Å—Ç—Ä–∏–º–∞"""
    global current_frame
    while True:
        with frame_lock:
            if current_frame is None:
                time.sleep(0.1)
                continue
            frame = current_frame.copy()
        
        ret, buffer = cv2.imencode('.jpg', frame)
        if not ret:
            continue
        
        yield (b'--frame\r\n'
               b'Content-Type: image/jpeg\r\n\r\n' + buffer.tobytes() + b'\r\n')
        time.sleep(0.033)


@app.route('/video_feed')
def video_feed():
    return Response(generate_frames(),
                    mimetype='multipart/x-mixed-replace; boundary=frame')


@app.route('/health')
def health():
    return {'status': 'ok', 'streaming': current_frame is not None}


def start_flask_server():
    """–ó–∞–ø—É—Å–∫ Flask —Å–µ—Ä–≤–µ—Ä–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ"""
    logger.info(f'Starting video stream server on port {config.video_stream_port}...')
    app.run(host='0.0.0.0', port=config.video_stream_port, threaded=True, 
            debug=False, use_reloader=False)


# ============================================================================
# MAIN
# ============================================================================

def main():
    global current_frame
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤
    known_embeddings, known_ids = load_employees()
    
    if not known_ids:
        logger.error('No employees with photos found!')
        return
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä—ã
    tracker = FaceTracker()
    presence_manager = PresenceManager(known_ids)
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º Flask –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
    flask_thread = threading.Thread(target=start_flask_server, daemon=True)
    flask_thread.start()
    logger.info(f'Video stream: http://localhost:{config.video_stream_port}/video_feed')
    
    # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ –∫–∞–º–µ—Ä–µ
    video_capture = connect_camera()
    
    frame_count = 0
    consecutive_failures = 0
    MAX_FAILURES = 10
    last_reload = time.time()
    
    logger.info('üé¨ Starting main loop...')
    
    try:
        while True:
            # –î–ª—è RTSP –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ–º –∑–∞–¥–µ—Ä–∂–∫—É
            camera_type = 'stream' if config.camera_source.startswith('rtsp://') else 'local'
            if camera_type == 'stream' and frame_count % 2 == 0:
                video_capture.grab()
            
            ret, frame = video_capture.read()
            
            if not ret:
                consecutive_failures += 1
                logger.warning(f'Failed to read frame ({consecutive_failures}/{MAX_FAILURES})')
                
                if consecutive_failures >= MAX_FAILURES:
                    logger.error('Too many failures, reconnecting...')
                    video_capture.release()
                    time.sleep(2)
                    video_capture = connect_camera()
                    consecutive_failures = 0
                else:
                    time.sleep(0.5)
                continue
            
            consecutive_failures = 0
            
            # –ì–æ—Ä—è—á–∞—è –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤
            if time.time() - last_reload > config.reload_employees_interval:
                logger.info('Reloading employees...')
                try:
                    new_embeddings, new_ids = load_employees()
                    if new_ids:
                        known_embeddings, known_ids = new_embeddings, new_ids
                        
                        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã—Ö –≤ presence manager
                        for emp_id in new_ids:
                            presence_manager.add_employee(emp_id)
                        
                        logger.info(f'Reloaded {len(new_ids)} employees')
                    last_reload = time.time()
                except Exception as e:
                    logger.error(f'Reload failed: {e}')
                    last_reload = time.time()
            
            frame_count += 1
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –∫–∞–∂–¥—ã–π N-–π –∫–∞–¥—Ä
            if frame_count % config.frame_skip != 0:
                # –û–±–Ω–æ–≤–ª—è–µ–º –≤–∏–¥–µ–æ —Å—Ç—Ä–∏–º
                with frame_lock:
                    current_frame = frame.copy()
                continue
            
            # –î–µ—Ç–µ–∫—Ü–∏—è –∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ
            faces = face_app.get(frame)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Ç—Ä–µ–∫–∏
            recognized_tracks = tracker.update(faces, known_embeddings, known_ids)
            
            # –ü–æ–ª—É—á–∞–µ–º ID —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã—Ö —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤
            recognized_emp_ids = [t.recognized_employee_id for t in recognized_tracks]
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏—è
            events = presence_manager.update(recognized_emp_ids)
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–±—ã—Ç–∏—è
            for emp_id, event_type in events:
                send_event(emp_id, event_type)
            
            # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
            display_frame = frame.copy()
            
            # –°—Ç–∞—Ç—É—Å –≤ —É–≥–ª—É
            preprocessing_status = "CLAHE‚ÜíSharp‚Üí" if config.enable_preprocessing else ""
            status_text = (
                f"{preprocessing_status}InsightFace | "
                f"Tracks: {len(tracker.tracks)} | "
                f"Recognized: {len(recognized_emp_ids)}"
            )
            cv2.putText(display_frame, status_text, (10, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 3)
            cv2.putText(display_frame, status_text, (10, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
            
            # –†–∏—Å—É–µ–º —Ä–∞–º–∫–∏
            for track in recognized_tracks:
                if track.last_bbox is not None:
                    x1, y1, x2, y2 = track.last_bbox.astype(int)
                    
                    # –ó–µ–ª—ë–Ω–∞—è —Ä–∞–º–∫–∞ –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã—Ö
                    cv2.rectangle(display_frame, (x1, y1), (x2, y2), (0, 255, 0), 3)
                    
                    # Label —Å ID –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
                    label = f"ID: {track.recognized_employee_id} ({track.recognition_confidence:.0%})"
                    cv2.rectangle(display_frame, (x1, y2 - 30), (x2, y2), (0, 255, 0), cv2.FILLED)
                    cv2.putText(display_frame, label, (x1 + 6, y2 - 8),
                               cv2.FONT_HERSHEY_DUPLEX, 0.5, (255, 255, 255), 1)
            
            # –ù–µ—Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–µ –ª–∏—Ü–∞ (—Ç—Ä–µ–∫–∏ –±–µ–∑ employee_id)
            for track in tracker.tracks:
                if track.recognized_employee_id is None and track.last_bbox is not None:
                    x1, y1, x2, y2 = track.last_bbox.astype(int)
                    cv2.rectangle(display_frame, (x1, y1), (x2, y2), (0, 0, 255), 3)
                    
                    label = f"Track {track.track_id} ({len(track.embeddings)}/{config.min_good_embeddings_per_track})"
                    cv2.rectangle(display_frame, (x1, y2 - 30), (x2, y2), (0, 0, 255), cv2.FILLED)
                    cv2.putText(display_frame, label, (x1 + 6, y2 - 8),
                               cv2.FONT_HERSHEY_DUPLEX, 0.5, (255, 255, 255), 1)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è —Å—Ç—Ä–∏–º–∞
            with frame_lock:
                current_frame = display_frame
            
            time.sleep(0.05)
    
    finally:
        video_capture.release()
        logger.info('Camera released')


if __name__ == '__main__':
    logger.info('='*60)
    logger.info('Recognition Service - AI-Powered')
    logger.info('='*60)
    logger.info(f'Backend: {config.backend_url}')
    logger.info(f'Camera: {config.camera_source}')
    logger.info(f'Preprocessing: {config.enable_preprocessing}')
    logger.info(f'Min face height: {config.min_face_height_pixels}px')
    logger.info(f'Min blur variance: {config.min_blur_variance}')
    logger.info(f'Embeddings per track: {config.min_good_embeddings_per_track}')
    logger.info('='*60)
    
    main()

