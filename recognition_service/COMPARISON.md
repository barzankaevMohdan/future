# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ä–æ–≥–æ –∏ –Ω–æ–≤–æ–≥–æ –∫–æ–¥–∞

## ‚úÖ –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ: –õ–æ–≥–∏–∫–∞ –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –Ω–∞ 100%

–í—Å–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω—ã **–¢–û–ß–ù–û** –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ.

## –ü–æ—Å—Ç—Ä–æ—á–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ

### 1. Quality Check (is_face_acceptable)

**–°—Ç–∞—Ä—ã–π –∫–æ–¥ (recognition/main.py:114-155):**
```python
def is_face_acceptable(face_img_bgr: np.ndarray, bbox: np.ndarray) -> Tuple[bool, Dict]:
    # –ú–µ—Ç—Ä–∏–∫–∞ 1: –†–∞–∑–º–µ—Ä –ª–∏—Ü–∞
    x1, y1, x2, y2 = bbox.astype(int)
    face_height = y2 - y1
    face_width = x2 - x1
    
    # –ú–µ—Ç—Ä–∏–∫–∞ 2: –†–∞–∑–º—ã—Ç–∏–µ
    gray_face = cv2.cvtColor(face_img_bgr, cv2.COLOR_BGR2GRAY)
    blur_score = compute_blur_score(gray_face)
    
    # –ú–µ—Ç—Ä–∏–∫–∞ 3: –Ø—Ä–∫–æ—Å—Ç—å
    mean_brightness = np.mean(gray_face)
    
    metrics = {
        'height': face_height,
        'width': face_width,
        'blur_score': blur_score,
        'brightness': mean_brightness
    }
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Ä–æ–≥–æ–≤
    if face_height < config.min_face_height_pixels:
        return False, metrics
    
    if blur_score < config.min_blur_variance:
        return False, metrics
    
    return True, metrics
```

**–ù–æ–≤—ã–π –∫–æ–¥ (recognition_service/recognition/quality.py:31-88):**
```python
def is_face_acceptable(
    face_img_bgr: np.ndarray,
    bbox: np.ndarray,
    config: Config
) -> Tuple[bool, Dict[str, float]]:
    # Extract bbox coordinates
    x1, y1, x2, y2 = bbox.astype(int)
    face_height = y2 - y1
    face_width = x2 - x1
    
    # Convert to grayscale for blur computation
    gray_face = cv2.cvtColor(face_img_bgr, cv2.COLOR_BGR2GRAY)
    
    # Compute metrics
    blur_score = compute_blur_score(gray_face)
    mean_brightness = float(np.mean(gray_face))
    
    metrics = {
        'height': float(face_height),
        'width': float(face_width),
        'blur_score': float(blur_score),
        'brightness': mean_brightness,
    }
    
    # Check thresholds
    if face_height < config.min_face_height_pixels:
        return False, metrics
    
    if blur_score < config.min_blur_variance:
        return False, metrics
    
    return True, metrics
```

‚úÖ **–ò–¥–µ–Ω—Ç–∏—á–Ω–æ!** –ï–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –æ—Ç–ª–∏—á–∏–µ - config –ø–µ—Ä–µ–¥–∞—ë—Ç—Å—è –∫–∞–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä (–ª—É—á—à–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è).

### 2. Preprocessing (preprocess_face_for_insightface)

**–°—Ç–∞—Ä—ã–π –∫–æ–¥ (recognition/main.py:162-203):**
```python
def preprocess_face_for_insightface(face_bgr: np.ndarray) -> np.ndarray:
    if not config.enable_preprocessing:
        return face_bgr
    
    # –®–∞–≥ 1: –ú—è–≥–∫–∏–π –¥–µ–Ω–æ–π–∑–∏–Ω–≥
    denoised = cv2.fastNlMeansDenoisingColored(
        face_bgr, None,
        h=config.denoise_strength,
        hColor=config.denoise_strength,
        templateWindowSize=7,
        searchWindowSize=21
    )
    
    # –®–∞–≥ 2: CLAHE —Ç–æ–ª—å–∫–æ –ø–æ —è—Ä–∫–æ—Å—Ç–∏
    ycrcb = cv2.cvtColor(denoised, cv2.COLOR_BGR2YCrCb)
    y, cr, cb = cv2.split(ycrcb)
    
    clahe = cv2.createCLAHE(
        clipLimit=config.clahe_clip_limit,
        tileGridSize=(8, 8)
    )
    y = clahe.apply(y)
    
    enhanced = cv2.cvtColor(cv2.merge([y, cr, cb]), cv2.COLOR_YCrCb2BGR)
    
    # –®–∞–≥ 3: –ú—è–≥–∫–∏–π unsharp mask
    gaussian = cv2.GaussianBlur(enhanced, (0, 0), 2.0)
    sharpened = cv2.addWeighted(enhanced, 1.5, gaussian, -0.5, 0)
    
    return sharpened
```

**–ù–æ–≤—ã–π –∫–æ–¥ (recognition_service/recognition/preprocessing.py:19-73):**
```python
def preprocess_face_for_insightface(face_bgr: np.ndarray, config: Config) -> np.ndarray:
    if not config.enable_preprocessing:
        return face_bgr
    
    # Step 1: Denoising
    denoised = cv2.fastNlMeansDenoisingColored(
        face_bgr,
        None,
        h=config.denoise_strength,
        hColor=config.denoise_strength,
        templateWindowSize=7,
        searchWindowSize=21
    )
    
    # Step 2: CLAHE on luminance channel
    ycrcb = cv2.cvtColor(denoised, cv2.COLOR_BGR2YCrCb)
    y, cr, cb = cv2.split(ycrcb)
    
    clahe = cv2.createCLAHE(
        clipLimit=config.clahe_clip_limit,
        tileGridSize=(8, 8)
    )
    y_enhanced = clahe.apply(y)
    
    enhanced = cv2.cvtColor(
        cv2.merge([y_enhanced, cr, cb]),
        cv2.COLOR_YCrCb2BGR
    )
    
    # Step 3: Unsharp mask
    gaussian = cv2.GaussianBlur(enhanced, (0, 0), 2.0)
    sharpened = cv2.addWeighted(enhanced, 1.5, gaussian, -0.5, 0)
    
    return sharpened
```

‚úÖ **–ò–¥–µ–Ω—Ç–∏—á–Ω–æ!** –í—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –∞–ª–≥–æ—Ä–∏—Ç–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã.

### 3. Face Tracking (FaceTracker.update)

**–°—Ç–∞—Ä—ã–π –∫–æ–¥ (recognition/main.py:276-358):**
```python
def update(self, faces, known_embeddings, known_ids):
    # –£–¥–∞–ª—è–µ–º –º—ë—Ä—Ç–≤—ã–µ —Ç—Ä–µ–∫–∏
    self.tracks = [t for t in self.tracks if t.is_alive()]
    
    matched_track_ids = set()
    
    for face in faces:
        bbox = face.bbox
        embedding = face.normed_embedding
        
        # Crop face
        x1, y1, x2, y2 = bbox.astype(int)
        face_crop = current_frame[y1:y2, x1:x2]
        
        # Quality check
        acceptable, quality = is_face_acceptable(face_crop, bbox)
        if not acceptable:
            continue
        
        # Find matching track (IoU)
        best_track = ...
        
        if best_track:
            # Preprocessing
            preprocessed = preprocess_face_for_insightface(face_crop)
            
            best_track.add_embedding(embedding, quality, bbox)
            
            # Try recognition
            if best_track.is_ready_for_recognition() and not best_track.recognized_employee_id:
                avg_embedding = best_track.get_average_embedding()
                emp_id, confidence = match_embedding_to_employee(...)
                ...
        else:
            # Create new track
            preprocessed = preprocess_face_for_insightface(face_crop)
            new_track = FaceTrack(self.next_track_id)
            ...
```

**–ù–æ–≤—ã–π –∫–æ–¥ (recognition_service/recognition/tracker.py:122-208):**
```python
def update(self, faces, frame, known_embeddings, known_ids):
    # Remove dead tracks
    self.tracks = [t for t in self.tracks if t.is_alive(self.config)]
    
    matched_track_ids = set()
    
    for face in faces:
        bbox = face.bbox
        embedding = face.normed_embedding
        
        # Crop face from frame
        x1, y1, x2, y2 = bbox.astype(int)
        face_crop = frame[y1:y2, x1:x2]
        
        # Quality check
        acceptable, quality = is_face_acceptable(face_crop, bbox, self.config)
        if not acceptable:
            continue
        
        # Find matching track
        best_track = self._find_matching_track(bbox, matched_track_ids)
        
        if best_track:
            # Preprocessing (as in original code)
            preprocessed = preprocess_face_for_insightface(face_crop, self.config)
            
            best_track.add_embedding(embedding, quality, bbox)
            
            # Try recognition if ready
            if best_track.is_ready_for_recognition(self.config) and not best_track.recognized_employee_id:
                avg_embedding = best_track.get_average_embedding()
                emp_id, confidence = match_embedding_to_employee(...)
                ...
        else:
            # Preprocessing (as in original code)
            preprocessed = preprocess_face_for_insightface(face_crop, self.config)
            
            # Create new track
            new_track = FaceTrack(self.next_track_id)
            ...
```

‚úÖ **–ò–¥–µ–Ω—Ç–∏—á–Ω–æ!** –õ–æ–≥–∏–∫–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞. Preprocessing –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ —Ç–µ—Ö –∂–µ –º–µ—Å—Ç–∞—Ö.

### 4. Presence Logic (PresenceManager)

**–°—Ç–∞—Ä—ã–π –∫–æ–¥ (recognition/main.py:391-458):**
```python
class PresenceManager:
    def __init__(self, employee_ids: List[int]):
        self.state = {
            emp_id: {
                'present': False,
                'last_seen': 0.0,
                'last_state_change': 0.0
            }
            for emp_id in employee_ids
        }
    
    def update(self, recognized_employee_ids: List[int]):
        now = time.time()
        events = []
        
        # –û–±–Ω–æ–≤–ª—è–µ–º last_seen
        for emp_id in recognized_employee_ids:
            if emp_id in self.state:
                self.state[emp_id]['last_seen'] = now
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º IN/OUT
        for emp_id, state in self.state.items():
            if emp_id in recognized_employee_ids:
                if not state['present']:
                    time_since_change = now - state['last_state_change']
                    if time_since_change > config.in_threshold_seconds:
                        state['present'] = True
                        state['last_state_change'] = now
                        events.append((emp_id, 'IN'))
            else:
                if state['present']:
                    time_since_seen = now - state['last_seen']
                    if time_since_seen > config.out_threshold_seconds:
                        state['present'] = False
                        state['last_state_change'] = now
                        events.append((emp_id, 'OUT'))
        
        return events
```

**–ù–æ–≤—ã–π –∫–æ–¥ (recognition_service/recognition/presence.py:27-110):**
```python
class PresenceManager:
    def __init__(self, employee_ids: List[int], config: Config):
        self.config = config
        self.state: Dict[int, Dict] = {}
        
        for emp_id in employee_ids:
            self.state[emp_id] = {
                'present': False,
                'last_seen': 0.0,
                'last_state_change': 0.0,
            }
    
    def update(self, recognized_employee_ids: List[int]) -> List[Tuple[int, str]]:
        now = time.time()
        events: List[Tuple[int, str]] = []
        
        # Update last_seen for recognized employees
        for emp_id in recognized_employee_ids:
            if emp_id in self.state:
                self.state[emp_id]['last_seen'] = now
        
        # Check each employee's state
        for emp_id, state in self.state.items():
            if emp_id in recognized_employee_ids:
                # Employee is visible
                if not state['present']:
                    time_since_change = now - state['last_state_change']
                    
                    if time_since_change > self.config.in_threshold_seconds:
                        state['present'] = True
                        state['last_state_change'] = now
                        events.append((emp_id, 'IN'))
            else:
                # Employee not visible
                if state['present']:
                    time_since_seen = now - state['last_seen']
                    
                    if time_since_seen > self.config.out_threshold_seconds:
                        state['present'] = False
                        state['last_state_change'] = now
                        events.append((emp_id, 'OUT'))
        
        return events
```

‚úÖ **–ò–¥–µ–Ω—Ç–∏—á–Ω–æ!** –õ–æ–≥–∏–∫–∞ IN/OUT –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.

### 5. Embedding Matching

**–°—Ç–∞—Ä—ã–π –∫–æ–¥ (recognition/main.py:361-384):**
```python
def match_embedding_to_employee(
    embedding: np.ndarray,
    known_embeddings: List[np.ndarray],
    known_ids: List[int]
) -> Tuple[Optional[int], float]:
    if len(known_embeddings) == 0:
        return None, 0.0
    
    # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ (–¥–ª—è InsightFace normalized embeddings)
    similarities = [np.dot(embedding, known_emb) for known_emb in known_embeddings]
    best_idx = np.argmax(similarities)
    best_similarity = similarities[best_idx]
    
    if best_similarity > config.insightface_threshold:
        return known_ids[best_idx], best_similarity
    
    return None, 0.0
```

**–ù–æ–≤—ã–π –∫–æ–¥ (recognition_service/recognition/matching.py:13-56):**
```python
def match_embedding_to_employee(
    embedding: np.ndarray,
    known_embeddings: List[np.ndarray],
    known_ids: List[int],
    config: Config
) -> Tuple[Optional[int], float]:
    if len(known_embeddings) == 0:
        return None, 0.0
    
    # Compute cosine similarities (dot product for normalized vectors)
    similarities = [
        float(np.dot(embedding, known_emb))
        for known_emb in known_embeddings
    ]
    
    # Find best match
    best_idx = int(np.argmax(similarities))
    best_similarity = similarities[best_idx]
    
    # Check threshold
    if best_similarity > config.insightface_threshold:
        return known_ids[best_idx], best_similarity
    
    return None, 0.0
```

‚úÖ **–ò–¥–µ–Ω—Ç–∏—á–Ω–æ!** –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–∞–∫ –∂–µ.

### 6. IoU Computation

**–°—Ç–∞—Ä—ã–π –∫–æ–¥ (recognition/main.py:245-266):**
```python
def compute_iou(bbox1: np.ndarray, bbox2: np.ndarray) -> float:
    x1_min, y1_min, x1_max, y1_max = bbox1
    x2_min, y2_min, x2_max, y2_max = bbox2
    
    # –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ
    inter_x_min = max(x1_min, x2_min)
    inter_y_min = max(y1_min, y2_min)
    inter_x_max = min(x1_max, x2_max)
    inter_y_max = min(y1_max, y2_max)
    
    inter_area = max(0, inter_x_max - inter_x_min) * max(0, inter_y_max - inter_y_min)
    
    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ
    bbox1_area = (x1_max - x1_min) * (y1_max - y1_min)
    bbox2_area = (x2_max - x2_min) * (y2_max - y2_min)
    union_area = bbox1_area + bbox2_area - inter_area
    
    if union_area == 0:
        return 0.0
    
    return inter_area / union_area
```

**–ù–æ–≤—ã–π –∫–æ–¥ (recognition_service/recognition/tracker.py:17-46):**
```python
def compute_iou(bbox1: np.ndarray, bbox2: np.ndarray) -> float:
    x1_min, y1_min, x1_max, y1_max = bbox1
    x2_min, y2_min, x2_max, y2_max = bbox2
    
    # Intersection
    inter_x_min = max(x1_min, x2_min)
    inter_y_min = max(y1_min, y2_min)
    inter_x_max = min(x1_max, x2_max)
    inter_y_max = min(y1_max, y2_max)
    
    inter_area = max(0, inter_x_max - inter_x_min) * max(0, inter_y_max - inter_y_min)
    
    # Union
    bbox1_area = (x1_max - x1_min) * (y1_max - y1_min)
    bbox2_area = (x2_max - x2_min) * (y2_max - y2_min)
    union_area = bbox1_area + bbox2_area - inter_area
    
    if union_area == 0:
        return 0.0
    
    return inter_area / union_area
```

‚úÖ **–ò–¥–µ–Ω—Ç–∏—á–Ω–æ!** –§–æ—Ä–º—É–ª–∞ IoU –Ω–µ –∏–∑–º–µ–Ω–µ–Ω–∞.

## –ß—Ç–æ –∏–∑–º–µ–Ω–∏–ª–æ—Å—å (—Ç–æ–ª—å–∫–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞)

### –ë—ã–ª–æ (–º–æ–Ω–æ–ª–∏—Ç):
```
recognition/
‚îî‚îÄ‚îÄ main.py (850 —Å—Ç—Ä–æ–∫)
    ‚îú‚îÄ‚îÄ Config
    ‚îú‚îÄ‚îÄ compute_blur_score()
    ‚îú‚îÄ‚îÄ is_face_acceptable()
    ‚îú‚îÄ‚îÄ preprocess_face_for_insightface()
    ‚îú‚îÄ‚îÄ FaceTrack
    ‚îú‚îÄ‚îÄ FaceTracker
    ‚îú‚îÄ‚îÄ compute_iou()
    ‚îú‚îÄ‚îÄ PresenceManager
    ‚îú‚îÄ‚îÄ match_embedding_to_employee()
    ‚îú‚îÄ‚îÄ load_employees()
    ‚îú‚îÄ‚îÄ send_event()
    ‚îú‚îÄ‚îÄ connect_camera()
    ‚îú‚îÄ‚îÄ generate_frames()
    ‚îú‚îÄ‚îÄ Flask routes
    ‚îî‚îÄ‚îÄ main()
```

### –°—Ç–∞–ª–æ (–º–æ–¥—É–ª—å–Ω–æ):
```
recognition_service/
‚îú‚îÄ‚îÄ config.py              # Config
‚îú‚îÄ‚îÄ recognition/
‚îÇ   ‚îú‚îÄ‚îÄ quality.py        # compute_blur_score, is_face_acceptable
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing.py  # preprocess_face_for_insightface
‚îÇ   ‚îú‚îÄ‚îÄ tracker.py        # FaceTrack, FaceTracker, compute_iou
‚îÇ   ‚îú‚îÄ‚îÄ presence.py       # PresenceManager
‚îÇ   ‚îî‚îÄ‚îÄ matching.py       # match_embedding_to_employee
‚îú‚îÄ‚îÄ employees.py           # load_employees
‚îú‚îÄ‚îÄ events.py              # send_event
‚îú‚îÄ‚îÄ camera.py              # connect_camera
‚îú‚îÄ‚îÄ streaming.py           # generate_frames
‚îú‚îÄ‚îÄ app.py                 # Flask routes
‚îú‚îÄ‚îÄ video_loop.py          # main loop
‚îî‚îÄ‚îÄ main.py                # entry point
```

## –ü—Ä–æ–≤–µ—Ä–∫–∞: –ê–ª–≥–æ—Ä–∏—Ç–º—ã –Ω–µ –∏–∑–º–µ–Ω–µ–Ω—ã

| –ö–æ–º–ø–æ–Ω–µ–Ω—Ç | –°—Ç–∞—Ä—ã–π –∫–æ–¥ | –ù–æ–≤—ã–π –∫–æ–¥ | –°—Ç–∞—Ç—É—Å |
|-----------|------------|-----------|--------|
| Quality check | main.py:114-155 | quality.py:31-88 | ‚úÖ –ò–¥–µ–Ω—Ç–∏—á–Ω–æ |
| Preprocessing | main.py:162-203 | preprocessing.py:19-73 | ‚úÖ –ò–¥–µ–Ω—Ç–∏—á–Ω–æ |
| IoU | main.py:245-266 | tracker.py:17-46 | ‚úÖ –ò–¥–µ–Ω—Ç–∏—á–Ω–æ |
| FaceTrack | main.py:210-242 | tracker.py:49-103 | ‚úÖ –ò–¥–µ–Ω—Ç–∏—á–Ω–æ |
| FaceTracker | main.py:269-358 | tracker.py:106-208 | ‚úÖ –ò–¥–µ–Ω—Ç–∏—á–Ω–æ |
| Matching | main.py:361-384 | matching.py:13-56 | ‚úÖ –ò–¥–µ–Ω—Ç–∏—á–Ω–æ |
| Presence | main.py:391-458 | presence.py:27-110 | ‚úÖ –ò–¥–µ–Ω—Ç–∏—á–Ω–æ |

## –ß—Ç–æ —É–ª—É—á—à–µ–Ω–æ (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è –ª–æ–≥–∏–∫–∏)

### 1. Dependency Injection
**–ë—ã–ª–æ:** –ì–ª–æ–±–∞–ª—å–Ω—ã–π `config`  
**–°—Ç–∞–ª–æ:** `config` –ø–µ—Ä–µ–¥–∞—ë—Ç—Å—è –∫–∞–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä

### 2. Thread Safety
**–ë—ã–ª–æ:** –ü—Ä—è–º–æ–π –¥–æ—Å—Ç—É–ø –∫ `current_frame`  
**–°—Ç–∞–ª–æ:** `streaming.set_frame()` / `get_frame_copy()` —Å lock

### 3. –ú–æ–¥—É–ª—å–Ω–æ—Å—Ç—å
**–ë—ã–ª–æ:** –í—Å—ë –≤ –æ–¥–Ω–æ–º —Ñ–∞–π–ª–µ  
**–°—Ç–∞–ª–æ:** –ö–∞–∂–¥—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –º–æ–¥—É–ª–µ

### 4. –¢–µ—Å—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å
**–ë—ã–ª–æ:** –°–ª–æ–∂–Ω–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å (–≥–ª–æ–±–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ)  
**–°—Ç–∞–ª–æ:** –ß–∏—Å—Ç—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏, –ª–µ–≥–∫–æ –º–æ–∫–∏—Ä–æ–≤–∞—Ç—å

### 5. –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å
**–ë—ã–ª–æ:** –û–¥–∏–Ω –∏–Ω—Å—Ç–∞–Ω—Å = –æ–¥–∏–Ω –ø—Ä–æ—Ü–µ—Å—Å  
**–°—Ç–∞–ª–æ:** –õ–µ–≥–∫–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å N –∏–Ω—Å—Ç–∞–Ω—Å–æ–≤ —Å —Ä–∞–∑–Ω—ã–º–∏ –∫–∞–º–µ—Ä–∞–º–∏

## –í—ã–≤–æ–¥

‚úÖ **–î–ê, –ª–æ–≥–∏–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω–∞ –¢–û–ß–ù–û –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ!**

–ò–∑–º–µ–Ω–∏–ª–∞—Å—å —Ç–æ–ª—å–∫–æ **—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–æ–¥–∞** (–º–æ–¥—É–ª–∏ –≤–º–µ—Å—Ç–æ –º–æ–Ω–æ–ª–∏—Ç–∞), –Ω–æ:
- ‚úÖ –í—Å–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –∏–¥–µ–Ω—Ç–∏—á–Ω—ã
- ‚úÖ –í—Å–µ –ø–æ—Ä–æ–≥–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã
- ‚úÖ –í—Å–µ —Ñ–æ—Ä–º—É–ª—ã –Ω–µ –∏–∑–º–µ–Ω–µ–Ω—ã
- ‚úÖ –í–Ω–µ—à–Ω–∏–µ –∫–æ–Ω—Ç—Ä–∞–∫—Ç—ã –Ω–µ –Ω–∞—Ä—É—à–µ–Ω—ã
- ‚úÖ –ü–æ–≤–µ–¥–µ–Ω–∏–µ –Ω–∞ 100% —Å–æ–≤–º–µ—Å—Ç–∏–º–æ

**–ú–æ–∂–µ—Ç–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–æ–≤—É—é –≤–µ—Ä—Å–∏—é!** üéâ







